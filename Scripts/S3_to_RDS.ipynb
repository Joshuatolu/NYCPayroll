{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# NYC Job to Load S3 Data to Staging DB - RDS PostgreSQL\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.7 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 2880 minutes.\n",
						"Setting Glue version to: 5.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 5\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 5\n",
						"Idle Timeout: 2880\n",
						"Session ID: ce15daf1-2a75-4e37-bbe4-dca24ed6303e\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.7\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session ce15daf1-2a75-4e37-bbe4-dca24ed6303e to get into ready status...\n",
						"Session ce15daf1-2a75-4e37-bbe4-dca24ed6303e has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 5.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"import boto3\n",
				"import os\n",
				"import json\n",
				"from pyspark.sql import SparkSession\n",
				"from pyspark.sql.functions import coalesce, col, round, to_date\n",
				"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
				"\n",
				"# Initialize a GlueContext\n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"\n",
				"s3_client = boto3.client('s3')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Setup necessary place holders and configuration details\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Downloaded env_file/config.json from S3 bucket cnytolubckt to config.json\n"
					]
				}
			],
			"source": [
				"# S3 path to load\n",
				"\n",
				"source_files = [\n",
				"    's3://xxxx/nyc_sourcedata/PayrollData/',\n",
				"    's3://xxxx/nyc_sourcedata/EmployeeData/',\n",
				"    's3://xxxx/nyc_sourcedata/TitleData/',\n",
				"    's3://xxxx/nyc_sourcedata/AgencyData/'\n",
				"]\n",
				"\n",
				"archive_files = [\n",
				"    's3://xxxx/nyc_archivedata/PayrollData/',\n",
				"    's3://xxxx/nyc_archivedata/EmployeeData/',\n",
				"    's3://xxxx/nyc_archivedata/TitleData/',\n",
				"    's3://xxxx/nyc_archivedata/AgencyData/'\n",
				"]\n",
				"\n",
				"bucket_name = 'xxxx'\n",
				"s3_file_path = 'env_file/config.json'\n",
				"local_file_path = 'config.json'\n",
				"\n",
				"# RDS connection details\n",
				"try:\n",
				"    s3_client.download_file(bucket_name, s3_file_path, local_file_path)\n",
				"    print(f\"Downloaded {s3_file_path} from S3 bucket {bucket_name} to {local_file_path}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error downloading file from S3: {e}\")\n",
				"    raise\n",
				"\n",
				"with open(local_file_path) as config_file:\n",
				"    config = json.load(config_file)\n",
				"\n",
				"rds_user = config['rds_user']\n",
				"rds_password = config['rds_password']\n",
				"rds_host = config['rds_host']\n",
				"rds_port = config['rds_port']\n",
				"rds_db_name = config['rds_db_name']\n",
				"rds_schema = config.get('rds_schema')\n",
				"\n",
				"# JDBC URL for PostgreSQL\n",
				"jdbc_url = f'jdbc:postgresql://{rds_host}:{rds_port}/{rds_db_name}'\n",
				"\n",
				"# Valid Table Details\n",
				"employee_table_name = 'Employee'\n",
				"payroll_table_name = 'Payroll'\n",
				"title_table_name = 'Title'\n",
				"agency_table_name = 'Agency'\n",
				"\n",
				"full_employee_table_name = f\"{rds_schema}.{employee_table_name}\"\n",
				"full_payroll_table_name = f\"{rds_schema}.{payroll_table_name}\"\n",
				"full_title_table_name = f\"{rds_schema}.{title_table_name}\"\n",
				"full_agency_table_name = f\"{rds_schema}.{agency_table_name}\"\n",
				"\n",
				"# Invalid Table Details\n",
				"payroll_issue = 'payroll_data_issues'\n",
				"payroll_issues = f\"{rds_schema}.{payroll_issue}\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Read the Data in S3 and convert to AWS DynamicFrame"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
					]
				}
			],
			"source": [
				"dynamic_frames = {}\n",
				"data_frames = {}\n",
				"\n",
				"for i, path in enumerate(source_files, start=1):\n",
				"    # Create each DynamicFrame\n",
				"    dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
				"        connection_type=\"s3\",\n",
				"        connection_options={\"paths\": [path]},\n",
				"        format=\"csv\",\n",
				"        format_options={\"withHeader\": True}\n",
				"    )\n",
				"    \n",
				"    # Store each AWS DynamicFrame in the dictionary\n",
				"    dynamic_frames[f\"dynamic_frame_{i}\"] = dynamic_frame\n",
				"    \n",
				"    # Convert to python DataFrame and store in the dictionary\n",
				"    data_frames[f\"dataframe_{i}\"] = dynamic_frame.toDF()\n",
				"    "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Convert aws DynamicFrame to python DataFrae"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Configure the JDBC properties"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Configure the JDBC properties\n",
				"jdbc_properties = {\n",
				"    \"user\": rds_user,\n",
				"    \"password\": rds_password,\n",
				"    \"driver\": \"org.postgresql.Driver\"\n",
				"}"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Transformations"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Payroll Transformation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"payroll_df = data_frames[\"dataframe_1\"]\n",
				"\n",
				"# Handle agencyid and agencycode columns\n",
				"if \"agencycode\" in payroll_df.columns and \"agencyid\" in payroll_df.columns:\n",
				"    # Both exist: merge into agencyid\n",
				"    payroll_df = payroll_df.withColumn('agencyid', coalesce(col(\"agencyid\"), col(\"agencycode\"))).drop(\"agencycode\")\n",
				"elif \"agencycode\" in payroll_df.columns:\n",
				"    # Only agencycode exists: rename to agencyid\n",
				"    payroll_df = payroll_df.withColumnRenamed(\"agencycode\", \"agencyid\")\n",
				"\n",
				"payroll_df = payroll_df.withColumn('fiscalyear', col('fiscalyear').cast('int')) \\\n",
				"                       .withColumn('payrollnumber', col('payrollnumber').cast('int')) \\\n",
				"                       .withColumn('agencyid', col('agencyid').cast('int')) \\\n",
				"                       .withColumn('agencyname', col('agencyid').cast('int')) \\\n",
				"                       .withColumn('employeeid', col('employeeid').cast('string')) \\\n",
				"                       .withColumn('lastname', col('lastname').cast('string')) \\\n",
				"                       .withColumn('firstname', col('firstname').cast('string')) \\\n",
				"                       .withColumn('agencystartdate', to_date(col('agencystartdate'), 'MM/dd/yyyy')) \\\n",
				"                       .withColumn('titlecode', col('titlecode').cast('string')) \\\n",
				"                       .withColumn('titledescription', col('titledescription').cast('string')) \\\n",
				"                       .withColumn('leavestatusasofjune30', col('leavestatusasofjune30').cast('string')) \\\n",
				"                       .withColumn('basesalary', round(col('basesalary').cast('double'), 2)) \\\n",
				"                       .withColumn('paybasis', col('paybasis').cast('string')) \\\n",
				"                       .withColumn('regularhours', col('regularhours').cast('double')) \\\n",
				"                       .withColumn('regulargrosspaid', round(col('regulargrosspaid').cast('double'), 2)) \\\n",
				"                       .withColumn('othours', col('othours').cast('double')) \\\n",
				"                       .withColumn('totalotpaid', round(col('totalotpaid').cast('double'), 2)) \\\n",
				"                       .withColumn('totalotherpay', round(col('totalotherpay').cast('double'), 2)) \\\n",
				"                       .withColumn('worklocationborough', col('worklocationborough').cast('string'))\n",
				"\n",
				"# Filter rows with NULL or 0 values in RegularGrossPaid or RegularHours\n",
				"payroll_invalid_df = payroll_df.filter(\n",
				"    (col(\"RegularGrossPaid\").isNull() | (col(\"RegularGrossPaid\") == 0)) |\n",
				"    (col(\"RegularHours\").isNull() | (col(\"RegularHours\") == 0))\n",
				")\n",
				"\n",
				"# Filter valid rows\n",
				"payroll_valid_df = payroll_df.filter(\n",
				"    ~((col(\"RegularGrossPaid\").isNull() | (col(\"RegularGrossPaid\") == 0)) |\n",
				"    (col(\"RegularHours\").isNull() | (col(\"RegularHours\") == 0)))\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Employee Transformation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"employee_df = data_frames[\"dataframe_2\"]\n",
				"\n",
				"employee_df = employee_df.withColumn(\"EmployeeID\", col(\"EmployeeID\").cast('string')) \\\n",
				"                         .withColumn(\"LastName\", col(\"LastName\").cast('string')) \\\n",
				"                         .withColumn(\"FirstName\", col(\"FirstName\").cast('string'))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Title Transformation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"title_df = data_frames[\"dataframe_3\"]\n",
				"\n",
				"title_df = title_df.withColumn(\"TitleCode\", col(\"TitleCode\").cast('string')) \\\n",
				"                   .withColumn(\"TitleDescription\", col(\"TitleDescription\").cast('string'))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### Agency Transformation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"agency_df = data_frames[\"dataframe_4\"]\n",
				"\n",
				"agency_df = agency_df.withColumn(\"AgencyID\", col(\"AgencyID\").cast('int')) \\\n",
				"                     .withColumn(\"AgencyName\", col(\"AgencyName\").cast('string'))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Write DataFrame to RDS using the JDBC connection"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Write DataFrame to RDS - Employee Table - using the JDBC connection\n",
				"try:\n",
				"    print(f\"Reading data from {source_files[1]}\")\n",
				"    print(f\"Connecting to RDS: {rds_host}\")\n",
				"    if not employee_df.rdd.isEmpty():\n",
				"        employee_df.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=full_employee_table_name,\n",
				"            mode=\"append\",\n",
				"            properties=jdbc_properties\n",
				"        )\n",
				"        print(f\"Valid Data successfully written to RDS table {full_employee_table_name}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error: {str(e)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Write DataFrame to RDS - Title Table - using the JDBC connection\n",
				"try:\n",
				"    print(f\"Reading data from {source_files[2]}\")\n",
				"    if not title_df.rdd.isEmpty():\n",
				"        title_df.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=full_title_table_name,\n",
				"            mode=\"append\",\n",
				"            properties=jdbc_properties\n",
				"        )\n",
				"        print(f\"Valid Data successfully written to RDS table {full_title_table_name}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error: {str(e)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Write DataFrame to RDS - Agency Table - using the JDBC connection\n",
				"try:\n",
				"    print(f\"Reading data from {source_files[3]}\")\n",
				"    if not agency_df.rdd.isEmpty():\n",
				"        agency_df.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=full_agency_table_name,\n",
				"            mode=\"append\",\n",
				"            properties=jdbc_properties\n",
				"        )\n",
				"        print(f\"Valid Data successfully written to RDS table {full_agency_table_name}\")\n",
				"except Exception as e:\n",
				"    print(f\"Error: {str(e)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Write DataFrame to RDS - Payroll Table - using the JDBC connection\n",
				"try:\n",
				"    print(f\"Reading data from {source_files[0]}\")\n",
				"    print(f\"Connecting to RDS: {rds_host}\")\n",
				"    if not payroll_valid_df.rdd.isEmpty():\n",
				"        payroll_valid_df.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=full_payroll_table_name,\n",
				"            mode=\"overwrite\",\n",
				"            properties=jdbc_properties\n",
				"        )\n",
				"        print(f\"Valid Data successfully written to RDS table {full_payroll_table_name}\")\n",
				"        \n",
				"    if not payroll_invalid_df.rdd.isEmpty():\n",
				"        payroll_invalid_df.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=payroll_issues,\n",
				"            mode=\"overwrite\",\n",
				"            properties=jdbc_properties\n",
				"        )\n",
				"        print(f\"Invalid Payroll Data successfully written to RDS table {payroll_issues}\")\n",
				"    \n",
				"except Exception as e:\n",
				"    print(f\"Error: {str(e)}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### House Keeping - Move processed files to Archive Folder"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Function to move files between folders\n",
				"def move_s3_files(source_folder, archive_folder):\n",
				"    # Extract bucket name and folder paths\n",
				"    bucket_name = source_folder.split('/')[2]\n",
				"    source_prefix = '/'.join(source_folder.split('/')[3:])\n",
				"    archive_prefix = '/'.join(archive_folder.split('/')[3:])\n",
				"    \n",
				"    # List all objects in the source folder\n",
				"    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=source_prefix)\n",
				"    if 'Contents' in response:\n",
				"        for obj in response['Contents']:\n",
				"            source_key = obj['Key']\n",
				"            \n",
				"            # Skip folders (S3 \"folders\" are keys that end with a '/')\n",
				"            if source_key.endswith('/'):\n",
				"                continue\n",
				"\n",
				"            file_name = os.path.basename(source_key)  # Get the file name\n",
				"            destination_key = f\"{archive_prefix}{file_name}\"\n",
				"\n",
				"            try:\n",
				"                # Copy the file to the archive folder\n",
				"                s3_client.copy_object(\n",
				"                    Bucket=bucket_name,\n",
				"                    CopySource={'Bucket': bucket_name, 'Key': source_key},\n",
				"                    Key=destination_key\n",
				"                )\n",
				"                \n",
				"                # Delete the file from the source folder\n",
				"                s3_client.delete_object(Bucket=bucket_name, Key=source_key)\n",
				"                print(f\"Moved {source_key} to {destination_key}\")\n",
				"            \n",
				"            except Exception as e:\n",
				"                print(f\"Error moving {source_key}: {str(e)}\")\n",
				"    else:\n",
				"        print(f\"No files found in {source_folder}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Run the function\n",
				"for source_folder, archive_folder in zip(source_files, archive_files):\n",
				"    move_s3_files(source_folder, archive_folder)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
